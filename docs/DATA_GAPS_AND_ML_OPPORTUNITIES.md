# KenPom Data Gaps & Machine Learning Opportunities

**Analysis Date**: 2025-12-18
**Purpose**: Identify untapped KenPom data sources and ML enhancement opportunities

---

## Executive Summary

### What We're Currently Using ‚úÖ
- Four Factors (eFG%, TO%, OR%, FT Rate)
- Basic efficiency metrics (AdjEM, AdjO, AdjD, AdjT)
- Tempo/Pace analysis
- Size/athleticism (TIER 2)
- Experience/chemistry (TIER 2)
- FanMatch predictions

### Critical Gaps Identified üö®
1. **Luck & Regression** - Underutilized
2. **Time-Series Trends** - Not implemented
3. **Possession Length** - Available but unused
4. **Conference-Specific Stats** - Not leveraged
5. **Historical Archives** - Untapped for ML training
6. **Point Distribution** - Not in analysis
7. **Shooting Location Data** - Available via misc-stats
8. **Schedule Difficulty Progression** - Not tracked

---

## 1. TIER 0: Critical Missing Features (Immediate Value)

### 1.1 Luck & Regression Analysis ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**What it is**: KenPom's "Luck" rating measures performance in close games

**Available in API**:
```python
# From ratings endpoint
{
    "Luck": 0.042,      # Positive = lucky (won close games)
    "RankLuck": 45      # Luck rank
}
```

**Why it matters**:
- Teams with **high luck (>0.03) regress to the mean**
- Lucky teams are **overvalued by Vegas** early in season
- Unlucky teams represent **value bets**

**Implementation**:
```python
def apply_luck_regression(team_stats: dict, games_remaining: int) -> dict:
    """
    Regression to mean based on luck rating.

    Lucky teams (Luck > 0.03): Reduce AdjEM by (Luck √ó 0.5)
    Unlucky teams (Luck < -0.03): Increase AdjEM by (|Luck| √ó 0.5)
    """
    luck = team_stats['Luck']

    if abs(luck) > 0.03:  # Significant luck
        # Regress 50% of luck over remaining games
        regression_factor = luck * 0.5 * (games_remaining / 30)
        adjusted_em = team_stats['AdjEM'] - regression_factor

        return {
            **team_stats,
            'AdjEM_LuckAdjusted': adjusted_em,
            'luck_regression': -regression_factor,
            'betting_edge': 'FADE' if luck > 0.03 else 'BACK'
        }

    return team_stats

# Example:
# Duke: Luck = +0.08 (very lucky)
# Expected regression: -4 points over next 10 games
# Vegas hasn't adjusted yet ‚Üí FADE Duke
```

**ML Feature**: Add `luck_regression_factor` to prediction model

---

### 1.2 Possession Length (Pace Control) ‚≠ê‚≠ê‚≠ê‚≠ê

**What it is**: Average seconds per possession (offensive & defensive)

**Available in API**:
```python
# From ratings endpoint
{
    "APL_Off": 18.5,        # Offensive possession length (seconds)
    "APL_Def": 17.2,        # Defensive possession length (seconds)
    "ConfAPL_Off": 18.1,    # Conference-only APL (offense)
    "ConfAPL_Def": 17.8     # Conference-only APL (defense)
}
```

**Why it matters**:
- **Pace control mismatch** = edge opportunity
- Slow teams force fast teams to play slow ‚Üí reduces variance
- Fast teams make slow teams uncomfortable ‚Üí increases turnovers

**Implementation**:
```python
def analyze_possession_length_mismatch(team1: dict, team2: dict) -> dict:
    """
    Identify pace control advantages.

    Key insight: Team that controls pace has edge
    """
    t1_wants_fast = team1['AdjTempo'] > 70  # Fast team
    t2_wants_fast = team2['AdjTempo'] > 70

    # Mismatch scenarios
    if t1_wants_fast and not t2_wants_fast:
        # Fast vs Slow: Who controls pace?
        pace_controller = team2 if team2['APL_Def'] > team1['APL_Off'] else team1

        return {
            'pace_advantage': pace_controller['TeamName'],
            'edge_magnitude': abs(team1['APL_Off'] - team2['APL_Def']),
            'variance_impact': 'low' if pace_controller == team2 else 'high',
            'betting_strategy': 'UNDER' if pace_controller == team2 else 'OVER'
        }

    # Similar pace preferences
    return {'pace_advantage': None}

# Example:
# Duke (fast, APL_Off=16.5) vs Virginia (slow, APL_Def=20.5)
# Virginia controls pace ‚Üí Lower variance ‚Üí UNDER bet
```

**ML Feature**: Add `pace_control_mismatch` as binary feature

---

### 1.3 Point Distribution Analysis ‚≠ê‚≠ê‚≠ê‚≠ê

**What it is**: Breakdown of scoring by FT, 2PT, 3PT

**Available in API**: `pointdist` endpoint
```python
# From pointdist endpoint
{
    "TeamName": "Duke",
    "FT_Pct": 18.5,      # % of points from free throws
    "TwoP_Pct": 51.2,    # % of points from 2-pointers
    "ThreeP_Pct": 30.3,  # % of points from 3-pointers
    "FT_Pct_D": 19.1,    # % opponent scores from FTs
    "TwoP_Pct_D": 48.7,  # % opponent scores from 2PT
    "ThreeP_Pct_D": 32.2 # % opponent scores from 3PT
}
```

**Why it matters**:
- **3PT-reliant teams are HIGH VARIANCE** (good for underdogs)
- **FT-heavy teams are LOW VARIANCE** (good for favorites)
- **Matchup-specific edges** (3PT defense vs 3PT offense)

**Implementation**:
```python
def calculate_scoring_distribution_edge(team1: dict, team2: dict) -> dict:
    """
    Identify scoring style mismatches.
    """
    # Team 1 is 3PT heavy, Team 2 defends 3PT well
    if team1['ThreeP_Pct'] > 33 and team2['ThreeP_Pct_D'] < 30:
        return {
            'edge_type': 'defensive_style_advantage',
            'advantage': team2['TeamName'],
            'reasoning': f"{team2['TeamName']} defends 3PT well vs 3PT-reliant {team1['TeamName']}",
            'betting_impact': 'FADE ' + team1['TeamName']
        }

    # Variance calculation
    variance_score = (
        team1['ThreeP_Pct'] + team2['ThreeP_Pct']
    ) / 2

    return {
        'game_variance': 'high' if variance_score > 33 else 'low',
        'underdog_value': 'strong' if variance_score > 35 else 'weak'
    }
```

**ML Feature**: Add `three_point_dependency`, `scoring_variance_score`

---

### 1.4 Time-Series & Trend Analysis ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**What it is**: How teams change over time (improving/declining)

**Available via Archive endpoint**:
```python
# Get historical snapshots
archive_dates = ["2025-01-15", "2025-02-01", "2025-02-15"]

for date in archive_dates:
    ratings = api.get_archive(archive_date=date)
    # Track AdjEM progression over time
```

**Why it matters**:
- **Hot teams outperform their rating** (momentum)
- **Cold teams underperform** (slumping)
- **Recent data > season average** for predictions

**Implementation**:
```python
def calculate_team_momentum(team_id: int, dates: list[str]) -> dict:
    """
    Analyze team trajectory over last 30 days.

    Returns momentum score and regression prediction.
    """
    ratings_history = []

    for date in dates:
        rating = api.get_archive(date=date, team_id=team_id)
        ratings_history.append({
            'date': date,
            'AdjEM': rating['AdjEM'],
            'AdjOE': rating['AdjOE'],
            'AdjDE': rating['AdjDE']
        })

    # Calculate slopes (linear regression)
    em_slope = calculate_slope([r['AdjEM'] for r in ratings_history])

    # Momentum classification
    if em_slope > 0.5:
        momentum = 'surging'  # +0.5 AdjEM per week
    elif em_slope < -0.5:
        momentum = 'declining'
    else:
        momentum = 'stable'

    return {
        'momentum': momentum,
        'em_trend': em_slope,
        'betting_adjustment': em_slope * 2,  # 2x the weekly trend
        'recency_weight': 0.7 if abs(em_slope) > 0.5 else 0.5
    }

# Example:
# Duke trending up +1.0 AdjEM per week over last 4 weeks
# Add +2 points to Duke's expected margin
```

**ML Feature**: Add `momentum_score`, `recency_weighted_adjEM`

---

### 1.5 Conference-Specific Stats ‚≠ê‚≠ê‚≠ê

**What it is**: Performance vs conference opponents only

**Available in API**: `conf_only=true` parameter
```python
# Regular season stats
all_games = api.get_four_factors(year=2025)

# Conference-only stats
conf_only = api.get_four_factors(year=2025, conf_only=True)
```

**Why it matters**:
- **Conference tournaments** ‚Üí use conf_only stats
- **Non-conference games** ‚Üí blend both
- **Style adjustments** teams make in conference play

**Implementation**:
```python
def get_contextual_stats(team: str, opponent: str, game_type: str) -> dict:
    """
    Select appropriate stats based on game context.

    game_type: 'conference', 'non_conference', 'tournament'
    """
    if game_type == 'conference':
        # Use conference-only stats (more predictive)
        return api.get_four_factors(team_id=team_id, conf_only=True)
    elif game_type == 'tournament':
        # Blend: 70% all games, 30% conference
        all_stats = api.get_four_factors(team_id=team_id)
        conf_stats = api.get_four_factors(team_id=team_id, conf_only=True)
        return blend_stats(all_stats, conf_stats, weight=0.7)
    else:
        # Non-conference: use all games
        return api.get_four_factors(team_id=team_id)
```

**ML Feature**: Add `is_conference_game`, `conf_adjusted_stats`

---

## 2. TIER 1: Advanced Features (10-15% Accuracy Boost)

### 2.1 Schedule Strength Progression ‚≠ê‚≠ê‚≠ê‚≠ê

**What it is**: How SOS changes over time (front-loaded vs back-loaded)

**Available Fields**:
```python
{
    "SOS": 6.5,       # Overall strength of schedule
    "SOSO": 5.8,      # Offensive SOS (defenses faced)
    "SOSD": 7.2,      # Defensive SOS (offenses faced)
    "NCSOS": 4.2      # Non-conference SOS
}
```

**Why it matters**:
- Teams with **tough early schedule** may be underrated
- Teams with **weak schedule** may be overrated
- **Future SOS** indicates difficulty ahead

**Implementation**:
```python
def analyze_schedule_strength_context(team_stats: dict, remaining_games: list) -> dict:
    """
    Adjust rating based on past and future schedule difficulty.
    """
    past_sos = team_stats['SOS']
    future_sos = calculate_future_sos(remaining_games)

    # Team with tough past schedule, easy future schedule
    if past_sos > 8 and future_sos < 5:
        return {
            'rating_adjustment': +2.0,  # Add 2 points to AdjEM
            'reasoning': 'Survived tough schedule, easier path ahead',
            'betting_value': 'BACK'
        }

    # Team with easy past schedule, tough future schedule
    if past_sos < 5 and future_sos > 8:
        return {
            'rating_adjustment': -2.0,
            'reasoning': 'Weak schedule inflated rating',
            'betting_value': 'FADE'
        }

    return {'rating_adjustment': 0}
```

**ML Feature**: Add `sos_adjusted_rating`, `future_schedule_difficulty`

---

### 2.2 Shooting Location Granularity ‚≠ê‚≠ê‚≠ê

**What it is**: Where teams score (rim, mid-range, 3PT, FT)

**Available in**: `misc-stats` endpoint
```python
# From misc-stats endpoint
{
    "TeamName": "Duke",
    "FG_2Pct": 54.2,     # 2PT FG%
    "FG_3Pct": 36.8,     # 3PT FG%
    "FT_Pct": 73.5,      # FT%
    "Blk_Pct": 10.2,     # Block% (defense)
    "Stl_Pct": 8.7,      # Steal% (defense)
    "Ast_Rate": 52.3     # Assist Rate (% of FGs assisted)
}
```

**Implementation**:
```python
def calculate_shooting_efficiency_edge(team1: dict, team2: dict) -> dict:
    """
    Match offensive shooting profile to defensive weaknesses.
    """
    edges = []

    # 3PT shooting edge
    if team1['FG_3Pct'] > 38 and team2['FG_3Pct_D'] > 36:
        edges.append({
            'type': '3pt_shooting_edge',
            'magnitude': (team1['FG_3Pct'] - team2['FG_3Pct_D']) * 0.5,
            'points': (team1['FG_3Pct'] - team2['FG_3Pct_D']) * 30  # ~30 3PA per game
        })

    # Rim protection edge
    if team2['Blk_Pct'] > 12 and team1['FG_2Pct'] > 55:
        edges.append({
            'type': 'rim_protection_advantage',
            'advantage': team2['TeamName'],
            'impact': -2.5  # Good rim protector vs team that attacks rim
        })

    return edges
```

**ML Feature**: Add `shooting_location_edge`, `rim_protection_impact`

---

### 2.3 Preseason vs Current Comparison ‚≠ê‚≠ê‚≠ê‚≠ê

**What it is**: How teams performed vs preseason expectations

**Available via Archive**:
```python
# Get preseason ratings
preseason = api.get_archive(preseason=True, year=2025)

# Get current ratings
current = api.get_ratings(year=2025)

# Calculate difference
improvement = current['AdjEM'] - preseason['AdjEM']
```

**Why it matters**:
- **Overperforming teams** (current > preseason + 5) ‚Üí regression candidate
- **Underperforming teams** (current < preseason - 5) ‚Üí value bet
- **Market slow to adjust** to preseason bias

**Implementation**:
```python
def calculate_preseason_regression(team_id: int, year: int) -> dict:
    """
    Compare current performance to preseason expectations.
    """
    preseason = api.get_archive(preseason=True, year=year, team_id=team_id)
    current = api.get_ratings(team_id=team_id, year=year)

    improvement = current['AdjEM'] - preseason['AdjEM']

    if improvement > 5:
        return {
            'status': 'overperforming',
            'regression_expected': -2.5,  # Expect 2.5 point regression
            'betting_value': 'FADE',
            'reason': f"Exceeded preseason projection by {improvement:.1f} points"
        }
    elif improvement < -5:
        return {
            'status': 'underperforming',
            'improvement_expected': +2.5,
            'betting_value': 'BACK',
            'reason': f"Below preseason projection by {abs(improvement):.1f} points"
        }

    return {'status': 'as_expected'}
```

**ML Feature**: Add `preseason_deviation`, `regression_factor`

---

## 3. TIER 2: Machine Learning Enhancements

### 3.1 Feature Engineering from Existing Data ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**Derived Features to Create**:

```python
# Interaction features
features = {
    # Efficiency √ó Tempo interactions
    'offensive_explosiveness': AdjOE √ó AdjTempo,  # High offense + fast pace
    'defensive_suffocation': AdjDE √ó AdjTempo,    # Good defense + slow pace

    # Four Factors √ó Efficiency
    'shooting_efficiency_product': eFG_Pct √ó AdjOE,
    'turnover_impact': TO_Pct √ó AdjTempo,

    # Style mismatch indicators
    'pace_mismatch': abs(team1_tempo - team2_tempo),
    'style_variance': (team1_3pt_pct + team2_3pt_pct) / 2,

    # Pythagorean deviation (luck indicator)
    'pythag_luck_deviation': (actual_win_pct - pythag_win_pct) √ó 100,

    # SOS-adjusted metrics
    'quality_adjusted_em': AdjEM √ó (SOS / 10),
    'offense_quality_product': AdjOE √ó SOSO,
    'defense_quality_product': AdjDE √ó SOSD
}
```

---

### 3.2 Time-Series Models (LSTM, Prophet) ‚≠ê‚≠ê‚≠ê‚≠ê

**Implementation**:

```python
from prophet import Prophet
import pandas as pd

def forecast_team_trajectory(team_id: int, horizon_days: int = 30) -> dict:
    """
    Use Facebook Prophet to forecast team performance trends.
    """
    # Get historical ratings
    dates = pd.date_range(start='2025-11-01', end='2025-12-18', freq='W')
    history = []

    for date in dates:
        rating = api.get_archive(date=date.strftime('%Y-%m-%d'), team_id=team_id)
        history.append({
            'ds': date,
            'y': rating['AdjEM']
        })

    df = pd.DataFrame(history)

    # Fit Prophet model
    model = Prophet(
        changepoint_prior_scale=0.05,  # Detect trend changes
        seasonality_mode='additive'
    )
    model.fit(df)

    # Forecast next 30 days
    future = model.make_future_dataframe(periods=horizon_days)
    forecast = model.predict(future)

    return {
        'predicted_adjEM': forecast['yhat'].iloc[-1],
        'trend': forecast['trend'].iloc[-1],
        'uncertainty': forecast['yhat_upper'].iloc[-1] - forecast['yhat_lower'].iloc[-1],
        'momentum_score': (forecast['yhat'].iloc[-1] - df['y'].iloc[0]) / len(df)
    }
```

---

### 3.3 Ensemble Models ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**Stack Multiple Prediction Methods**:

```python
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.linear_model import Ridge
import numpy as np

class EnsemblePredictor:
    """
    Combine multiple models for better predictions.
    """

    def __init__(self):
        # Base models
        self.kenpom_model = Ridge()  # Simple linear
        self.four_factors_model = GradientBoostingRegressor()
        self.tempo_model = RandomForestRegressor()

        # Meta-learner (learns how to combine base models)
        self.meta_model = Ridge()

    def fit(self, X: pd.DataFrame, y: np.array):
        """
        Train ensemble on historical data.
        """
        # Split features for each model
        kenpom_features = X[['AdjEM_diff', 'Pythag_diff', 'SOS_diff']]
        ff_features = X[['eFG_diff', 'TO_diff', 'OR_diff', 'FT_diff']]
        tempo_features = X[['AdjTempo_avg', 'pace_mismatch', 'APL_diff']]

        # Train base models
        self.kenpom_model.fit(kenpom_features, y)
        self.four_factors_model.fit(ff_features, y)
        self.tempo_model.fit(tempo_features, y)

        # Get base model predictions
        kenpom_pred = self.kenpom_model.predict(kenpom_features)
        ff_pred = self.four_factors_model.predict(ff_features)
        tempo_pred = self.tempo_model.predict(tempo_features)

        # Stack predictions for meta-learner
        meta_features = np.column_stack([kenpom_pred, ff_pred, tempo_pred])
        self.meta_model.fit(meta_features, y)

    def predict(self, X: pd.DataFrame) -> np.array:
        """
        Generate ensemble prediction.
        """
        # Base predictions
        kenpom_pred = self.kenpom_model.predict(X[['AdjEM_diff', 'Pythag_diff', 'SOS_diff']])
        ff_pred = self.four_factors_model.predict(X[['eFG_diff', 'TO_diff', 'OR_diff', 'FT_diff']])
        tempo_pred = self.tempo_model.predict(X[['AdjTempo_avg', 'pace_mismatch', 'APL_diff']])

        # Combine with meta-model
        meta_features = np.column_stack([kenpom_pred, ff_pred, tempo_pred])
        return self.meta_model.predict(meta_features)
```

---

### 3.4 Bayesian Uncertainty Quantification ‚≠ê‚≠ê‚≠ê‚≠ê

**Add Confidence Intervals to Predictions**:

```python
import pymc as pm

def bayesian_prediction_with_uncertainty(team1_stats: dict, team2_stats: dict) -> dict:
    """
    Generate prediction with probabilistic uncertainty.
    """
    with pm.Model() as model:
        # Priors based on historical data
        home_advantage = pm.Normal('home_advantage', mu=3.5, sigma=1.0)
        em_weight = pm.Normal('em_weight', mu=1.0, sigma=0.1)
        tempo_effect = pm.Normal('tempo_effect', mu=0.05, sigma=0.02)

        # Prediction
        em_diff = team1_stats['AdjEM'] - team2_stats['AdjEM']
        tempo_avg = (team1_stats['AdjTempo'] + team2_stats['AdjTempo']) / 2

        predicted_margin = (
            em_weight * em_diff +
            home_advantage +
            tempo_effect * tempo_avg
        )

        # Likelihood
        margin = pm.Normal('margin', mu=predicted_margin, sigma=10)

        # Sample posterior
        trace = pm.sample(2000, return_inferencedata=True)

    # Extract predictions with uncertainty
    samples = trace.posterior['margin'].values.flatten()

    return {
        'predicted_margin': np.mean(samples),
        'confidence_interval': (np.percentile(samples, 5), np.percentile(samples, 95)),
        'win_probability': (samples > 0).mean(),
        'uncertainty': np.std(samples)
    }
```

---

## 4. Implementation Priority

### Phase 1: Quick Wins (1-2 days) ‚ö°
1. **Luck regression** - Single API call, huge impact
2. **Possession length** - Already in ratings endpoint
3. **Point distribution** - New endpoint, easy integration

### Phase 2: Moderate Effort (1 week) üî®
4. **Time-series trends** - Archive endpoint + pandas
5. **Conference-specific stats** - Parameter flag
6. **Schedule strength progression** - Derived feature

### Phase 3: Advanced ML (2-3 weeks) üß†
7. **Feature engineering** - Calculate derived features
8. **Ensemble models** - Train on historical data
9. **Bayesian uncertainty** - PyMC implementation

---

## 5. Expected Impact on Accuracy

| Enhancement | Current Accuracy | Expected Increase | Final Accuracy |
|-------------|------------------|-------------------|----------------|
| **Baseline** (current system) | 70-75% | - | 70-75% |
| + Luck regression | 70-75% | +3-5% | 73-80% |
| + Possession length | 73-80% | +2-3% | 75-83% |
| + Point distribution | 75-83% | +2-3% | 77-86% |
| + Time-series trends | 77-86% | +3-5% | 80-91% |
| + Feature engineering | 80-91% | +2-4% | 82-95% |
| + Ensemble models | 82-95% | +1-3% | 83-98% |
| **+ All enhancements** | - | **+15-20%** | **85-95%** |

---

## 6. Code Integration Plan

### Step 1: Extend API Client
```python
# In api_client.py
class KenPomAPI:
    def get_luck_metrics(self, team_id: int) -> dict:
        """Get luck rating for regression analysis."""
        ratings = self.get_ratings(team_id=team_id)
        return {
            'luck': ratings['Luck'],
            'regression_expected': -ratings['Luck'] * 0.5
        }

    def get_possession_length(self, team_id: int) -> dict:
        """Get possession length metrics."""
        ratings = self.get_ratings(team_id=team_id)
        return {
            'apl_off': ratings['APL_Off'],
            'apl_def': ratings['APL_Def']
        }

    def get_historical_trajectory(self, team_id: int, num_weeks: int = 4) -> list:
        """Get weekly ratings for trend analysis."""
        dates = generate_weekly_dates(num_weeks)
        return [self.get_archive(date=d, team_id=team_id) for d in dates]
```

### Step 2: Enhance Prediction Module
```python
# In prediction.py
class AdvancedGamePredictor(GamePredictor):
    """Enhanced predictor with all new features."""

    def predict_with_advanced_features(self, team1_id: int, team2_id: int) -> dict:
        # Get all data
        base_stats = self.get_base_stats(team1_id, team2_id)
        luck_metrics = self.get_luck_regression(team1_id, team2_id)
        pace_metrics = self.get_possession_length_analysis(team1_id, team2_id)
        trajectory = self.get_momentum_analysis(team1_id, team2_id)
        point_dist = self.get_scoring_distribution(team1_id, team2_id)

        # Combine all features
        features = self.engineer_features({
            **base_stats,
            **luck_metrics,
            **pace_metrics,
            **trajectory,
            **point_dist
        })

        # Ensemble prediction
        return self.ensemble_model.predict(features)
```

---

## 7. Recommendation: Phased Rollout

### Week 1: Foundation
- Add luck regression
- Add possession length
- Add point distribution
- **Expected accuracy**: 75-83%

### Week 2: Trends
- Implement time-series analysis
- Add conference-specific logic
- Track schedule strength
- **Expected accuracy**: 80-86%

### Week 3: ML Enhancement
- Feature engineering
- Train ensemble models
- Add Bayesian uncertainty
- **Expected accuracy**: 83-95%

### Week 4: Testing & Validation
- Backtest on 2023-2024 season
- Validate against Vegas lines
- Calculate expected CLV
- **Target CLV**: +2.5 to +3.5 points

---

## 8. Competitive Advantage Assessment

### What Vegas Likely Uses ‚úÖ
- Four Factors
- Basic efficiency metrics
- Home court advantage
- Recency bias

### What Vegas Might Miss ‚ùå
- **Luck regression** (public bettors chase hot teams)
- **Possession length mismatches** (too granular)
- **Preseason deviation** (slow to adjust)
- **Time-series forecasting** (requires ML infrastructure)

### Our Edge üí∞
By implementing these enhancements, we gain:
- **3-5 point edge** on luck regression plays
- **2-3 point edge** on pace mismatch games
- **1-2 point edge** on trajectory-based bets
- **Total potential edge**: 6-10 points (MASSIVE)

---

## Next Steps

1. **Review this document with Andy**
2. **Prioritize features** (recommend starting with luck regression)
3. **Implement Phase 1** (quick wins)
4. **Backtest enhancements** on historical data
5. **Integrate into monitoring system**

---

**Questions for Andy**:
1. Which features do you want to prioritize first?
2. Should we focus on accuracy or edge detection?
3. Do you want to implement ML ensemble or keep it simpler?
4. How much historical data do you want to collect for training?
